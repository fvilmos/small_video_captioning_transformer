{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c6ea9e2",
   "metadata": {},
   "source": [
    "# Small Video Captioning transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb03926-8bf0-4b36-8d78-f4a17c40d5cb",
   "metadata": {},
   "source": [
    " Video captioning (describes the context of a video) using a processing efficient architecture.\n",
    " - small CNN or VisionTransformer as selectable backbone\n",
    " - encoder - decoder with text and vision information that generates in autoregressive manner the image captioning\n",
    "\n",
    "Author: fvilmos\n",
    "https://github.com/fvilmos\n",
    "\n",
    "See references:\n",
    "- Attention Is All You Need - https://arxiv.org/abs/1706.03762"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970c85a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "import argparse\n",
    "import os\n",
    "from PIL import Image\n",
    "from thop import profile\n",
    "\n",
    "from utils.vocabulary import Vocabulary\n",
    "from utils.msvd_dataset import MsvdDataset\n",
    "from utils.video_captioning_transformer import VideoCaptioner, generate_caption\n",
    "from utils.video_encoder import CNNEncoder, ViTEncoder, MobileNetV2Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70920e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=128\n",
    "IN_CHANNELS = 3\n",
    "IMG_SIZE = 224\n",
    "EPOCS = 20\n",
    "LR = 3e-4\n",
    "WEIGHT_DECAY=1e-4\n",
    "LOG_STEP = 20\n",
    "MAX_LEN= 100\n",
    "MAX_GEN = 50\n",
    "NR_OF_FRAMES = 8\n",
    "VISION_Encoder = MobileNetV2Encoder\n",
    "WORKERS=0\n",
    "\n",
    "save_path = \"./vct_model.pth\"\n",
    "root_dir = '../../datasets/msvd/YouTubeClips'\n",
    "annotation_file = '../../datasets/msvd/annotations.txt'\n",
    "voc_path = './voc.json'\n",
    "model_path = \"./vct_model.pth\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"Run on:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b2e2f8",
   "metadata": {},
   "source": [
    "### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4069cf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ann_dict(fname):\n",
    "    ann_dict = {}\n",
    "    with open(fname, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for i,l in enumerate(lines):\n",
    "            # slice line, get ID, rest dump as a caption, \n",
    "            # ignore the seme key if appears again\n",
    "            sl = l.strip().split(' ')\n",
    "            id = sl[0]\n",
    "            cap = sl[1:]\n",
    "            if id not in ann_dict:\n",
    "                ann_dict[i] = {'id':id, 'caption':cap}\n",
    "    return ann_dict\n",
    "    \n",
    "ann_dict = get_ann_dict(annotation_file)\n",
    "\n",
    "print (ann_dict[0], len(ann_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b1b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(ann_dict, threshold=3):\n",
    "    vocab = Vocabulary()\n",
    "    counter = {}\n",
    "    for k,v in ann_dict.items():\n",
    "        caption = \" \".join(v['caption'])\n",
    "        tokens = vocab.custom_word_tokenize(caption)\n",
    "        for word in tokens:\n",
    "            counter[word] = counter.get(word, 0) + 1\n",
    "\n",
    "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
    "\n",
    "    for word in words:\n",
    "        vocab.add_word(word)\n",
    "    return vocab\n",
    "\n",
    "voc = build_vocab(ann_dict=ann_dict, threshold=4)\n",
    "print (\"voc len ==>\",len(voc))\n",
    "\n",
    "# export voc\n",
    "voc.export_vocabulary(\"./voc.json\")\n",
    "\n",
    "print ('vocabulary exported!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab56c805-7096-4cc1-ae0a-65b89655a104",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a15a1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    Creates a mini-batch of tensors from a list of tuples.\n",
    "    \"\"\"\n",
    "    # Sort a data list by caption length (descending order).\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    videos, captions = zip(*data)\n",
    "\n",
    "    # Merge videos (from tuple of 3D tensor to 4D tensor).\n",
    "    videos = torch.stack(videos, 0)\n",
    "\n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]\n",
    "    return videos, targets, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5038101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "dataset = MsvdDataset(root=root_dir, annotation_dict=ann_dict, vocab=voc, transform=transform, num_frames=NR_OF_FRAMES)\n",
    "data_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS, collate_fn=collate_fn)\n",
    "\n",
    "videos, targets, lenghts = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff4fc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (videos.shape, targets.shape, len(lenghts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858c88e1-d08f-4826-8858-5782d9f75407",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6cff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VideoCaptioner(vocab_size=len(voc),\n",
    "                       dim=256,\n",
    "                       num_heads=4,\n",
    "                       num_layers=2,\n",
    "                       vis_out_dimension=1280,\n",
    "                       num_frames=NR_OF_FRAMES,\n",
    "                       max_len=MAX_LEN,\n",
    "                       vis_hxw_out = 49,\n",
    "                       VisionEncoder=VISION_Encoder).to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                              lr=LR,\n",
    "                              weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fb6293",
   "metadata": {},
   "source": [
    "### Model Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6666ff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size = len(voc)\n",
    "\n",
    "# B, NUM_FRAMES, C, H, W\n",
    "dummy_images = torch.randn(1,8, 3, IMG_SIZE, IMG_SIZE).to(device)\n",
    "\n",
    "# B, Nmax-1\n",
    "dummy_captions = torch.randint(0, voc_size, (1, MAX_LEN - 1)).long().to(device)\n",
    "\n",
    "# Profile the model using thop\n",
    "macs, params = profile(model, inputs=(dummy_images, dummy_captions))\n",
    "\n",
    "print(f\"Total MACs: {macs / 1e9:.2f} G, FLOPS: {2*macs / 1e9:.2f} G\")\n",
    "print(f\"Total Parameters: {params / 1e6:.2f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e1662e-1e3d-4b8f-a2ef-2aec2a2c6675",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a72284",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_loss = 1e+9\n",
    "\n",
    "# save the model\n",
    "def save_model(in_model,path):\n",
    "    torch.save(in_model.state_dict(), path)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCS):\n",
    "    for i, (videos, captions, lengths) in enumerate(data_loader):\n",
    "        model.train()\n",
    "        model.vision_encoder.vision_encoder.eval()\n",
    "\n",
    "        # Move data to device\n",
    "        videos = videos.to(device)\n",
    "        captions = captions.to(device)\n",
    "        targets = captions[:, 1:].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(videos, captions[:, :-1])\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = torch.nn.functional.cross_entropy(outputs.reshape(-1, len(voc)), targets.reshape(-1), ignore_index=voc('<pad>'))\n",
    "\n",
    "        if best_loss > loss.item():\n",
    "            save_model(model,save_path)\n",
    "            best_loss = loss.item()\n",
    "        \n",
    "        # backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # log progress\n",
    "        if i % LOG_STEP == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{EPOCS}], Step [{i}/{len(data_loader)}],',\n",
    "                  f'Loss: {loss.item():.4f}, Perplexity: {torch.exp(loss).item():.4f}')\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a43ae7",
   "metadata": {},
   "source": [
    "## Test Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d859d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = VideoCaptioner(vocab_size=len(voc),\n",
    "                            dim=256,\n",
    "                            num_heads=4,\n",
    "                            num_layers=2,\n",
    "                            vis_out_dimension=1280,\n",
    "                            vis_hxw_out=49,\n",
    "                            num_frames=NR_OF_FRAMES,\n",
    "                            max_len=MAX_LEN,\n",
    "                            VisionEncoder=VISION_Encoder).to(device)\n",
    "\n",
    "# load the trained model\n",
    "test_model.load_state_dict(torch.load(model_path, map_location=device), strict=False)\n",
    "\n",
    "# get a list of test videos\n",
    "test_video_paths = glob.glob(os.path.join(root_dir, '*.avi'))\n",
    "\n",
    "# generate captions for a few sample videos\n",
    "for video_path in test_video_paths[:5]:\n",
    "    print(\"video path:\", video_path)\n",
    "    dataset = MsvdDataset(root=root_dir, annotation_dict=ann_dict, vocab=voc, transform=transform, num_frames=NR_OF_FRAMES)\n",
    "    video = dataset._load_frames(video_path)\n",
    "    tvideo = torch.stack([transform(frame) for frame in video]).to(device)\n",
    "\n",
    "    # Generate caption\n",
    "    cap = generate_caption(test_model,tvideo, voc, max_len=MAX_GEN)\n",
    "    print(\"Caption:\", \" \".join(cap[0]))\n",
    "\n",
    "    plt.imshow(video[5])\n",
    "    plt.axis('off')\n",
    "    plt.title(\" \".join(cap[0][1:-1]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10eb8b8-f5d9-4a73-a18f-df7ba8b859c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
